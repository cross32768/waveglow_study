{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.0.0\n",
      "Is GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('Is GPU available:', use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32\n",
    "device = torch.device('cuda' if use_gpu else 'cpu')\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "if use_gpu:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成（シンプルな正弦波＋ノイズ）\n",
    "ϵ = 0.1\n",
    "datalength = 100\n",
    "t = torch.linspace(0, 1, datalength)\n",
    "y = torch.sin(2 * np.pi * t)\n",
    "\n",
    "datasize = 500\n",
    "data_tensor = torch.zeros(datasize, datalength)\n",
    "for i in range(datasize):\n",
    "    data_tensor[i] = y.clone() + ϵ*torch.randn(t.size())\n",
    "    \n",
    "train_loader = DataLoader(data_tensor, batch_size=batchsize, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invertibity check -> ok\n",
    "class Invertible1x1Conv(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(Invertible1x1Conv, self).__init__()\n",
    "        self.conv = nn.Conv1d(num_features, num_features, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        \n",
    "        W = torch.qr(torch.FloatTensor(num_features, num_features).normal_())[0]\n",
    "        \n",
    "        if torch.det(W) < 0:\n",
    "            W[:,0] = -1*W[:,0]\n",
    "        \n",
    "        self.conv.weight.data = W.view(num_features, num_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.conv(x)\n",
    "        log_det_jacobian = self.calculate_log_det_jacobian(x)\n",
    "        return z, log_det_jacobian\n",
    "        \n",
    "    def inverse(self, z, train_finished=False):\n",
    "        W = self.conv.weight.squeeze()\n",
    "        if train_finished:\n",
    "            if not hasattr(self, 'W_inverse'):\n",
    "                W_inverse = W.inverse()\n",
    "                self.W_inverse = W_inverse.view(*W_inverse.size(), 1)\n",
    "            x = F.conv1d(z, self.W_inverse, bias=None, stride=1, padding=0)\n",
    "        else:\n",
    "            W_inverse = W.inverse()\n",
    "            W_inverse = W_inverse.view(*W_inverse.size(), 1)\n",
    "            x = F.conv1d(z, W_inverse, bias=None, stride=1, padding=0)\n",
    "        return x\n",
    "        \n",
    "    def calculate_log_det_jacobian(self, x):\n",
    "        batch_size, group_size, n_of_groups = x.size()\n",
    "        W = self.conv.weight.squeeze()\n",
    "        log_det_jacobian = batch_size * n_of_groups * torch.logdet(W)\n",
    "        return log_det_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(NN, self).__init__()\n",
    "        n_hidden = 64\n",
    "        self.cv1 = nn.Conv1d(n_features, n_hidden, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden)\n",
    "        self.cv2 = nn.Conv1d(n_hidden, n_hidden, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(n_hidden)\n",
    "        self.cv3 = nn.Conv1d(n_hidden, 2*n_features, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.cv3.weight.data.zero_()\n",
    "        self.cv3.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.cv1(x)))\n",
    "        out = F.relu(self.bn2(self.cv2(out)))\n",
    "        out = self.cv3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveGlow(nn.Module):\n",
    "    def __init__(self, n_flows, n_group, n_early_every, n_early_size):\n",
    "        super(WaveGlow, self).__init__()\n",
    "        \n",
    "        assert(n_group % 2 == 0)\n",
    "        self.n_flows = n_flows\n",
    "        self.n_group = n_group\n",
    "        self.n_early_every = n_early_every\n",
    "        self.n_early_size = n_early_size\n",
    "        \n",
    "        self.NN = nn.ModuleList()\n",
    "        self.convinv = nn.ModuleList()\n",
    "        \n",
    "        n_half = int(n_group/2)\n",
    "        \n",
    "        n_remaining_channels = n_group\n",
    "        for k in range(n_flows):\n",
    "            if k % self.n_early_every == 0 and k > 0:\n",
    "                n_half = n_half - int(self.n_early_size/2)\n",
    "                n_remaining_channels = n_remaining_channels - self.n_early_size\n",
    "            self.convinv.append(Invertible1x1Conv(n_remaining_channels))\n",
    "            self.NN.append(NN(n_half))\n",
    "        self.n_remaining_channels = n_remaining_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unfold(1, self.n_group, self.n_group).permute(0, 2, 1)\n",
    "        z = []\n",
    "        log_det_jacobian = 0\n",
    "        \n",
    "        for k in range(self.n_flows):\n",
    "            if k % self.n_early_every == 0 and k > 0:\n",
    "                z.append(x[:,:self.n_early_size,:])\n",
    "                x = x[:,self.n_early_size:,:]\n",
    "            x, log_det_temp = self.convinv[k](x)\n",
    "            log_det_jacobian += log_det_temp\n",
    "            \n",
    "            n_half = int(x.size(1)/2)\n",
    "            x_0 = x[:,:n_half,:]\n",
    "            x_1 = x[:,n_half:,:]\n",
    "            \n",
    "            out = self.NN[k](x_0)\n",
    "            log_s = out[:,n_half:,:]\n",
    "            b = out[:,:n_half,:]\n",
    "            x_1 = torch.exp(log_s)*x_1 + b\n",
    "            \n",
    "            log_det_jacobian += torch.sum(log_s)\n",
    "        \n",
    "        z.append(x)\n",
    "        z = torch.cat(z, dim=1)\n",
    "        self.z_size = z.size()\n",
    "        return z, log_det_jacobian\n",
    "    \n",
    "    def infer(self, sigma=1.0):\n",
    "        z = sigma * torch.cuda.FloatTensor(self.z_size[0], \n",
    "                                           self.n_remaining_channels, \n",
    "                                           self.z_size[2]).normal_()\n",
    "        for k in reversed(range(self.n_flows)):\n",
    "            n_half = int(z.size(1)/2)\n",
    "            z_0 = z[:,:n_half,:]\n",
    "            z_1 = z[:,n_half:,:]\n",
    "            \n",
    "            out = self.NN[k](z_0)\n",
    "            log_s = out[:,n_half:,:]\n",
    "            b = out[:,:n_half,:]\n",
    "            z_1 = (z_1 - b) / torch.exp(log_s)\n",
    "            z = torch.cat([z_0, z_1], dim=1)\n",
    "            \n",
    "            z = self.convinv[k].inverse(z)\n",
    "            \n",
    "            if k % self.n_early_every == 0 and k > 0:\n",
    "                add_z = sigma * torch.cuda.FloatTensor(self.z_size[0], \n",
    "                                                       self.n_early_size, \n",
    "                                                       self.z_size[2]).normal_()\n",
    "                z = torch.cat([add_z, z], dim=1)\n",
    "        z = z.permute(0,2,1).contiguous().view(z.size(0), -1).data\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveGlowLoss(nn.Module):\n",
    "    def __init__(self, sigma=1.0):\n",
    "        super(WaveGlowLoss, self).__init__()\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def forward(self, model_output):\n",
    "        z, log_det_jacobian = model_output\n",
    "        batch_size, group_size, n_of_groups = z.size()\n",
    "        loss = torch.sum(z*z)/(2*self.sigma*self.sigma) - log_det_jacobian\n",
    "        \n",
    "        return loss / (batch_size*group_size*n_of_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of trainable parameters: 113176\n",
      "\n",
      "Model:\n",
      " WaveGlow(\n",
      "  (NN): ModuleList(\n",
      "    (0): NN(\n",
      "      (cv1): Conv1d(4, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cv3): Conv1d(64, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (1): NN(\n",
      "      (cv1): Conv1d(4, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cv3): Conv1d(64, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (2): NN(\n",
      "      (cv1): Conv1d(3, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cv3): Conv1d(64, 6, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (3): NN(\n",
      "      (cv1): Conv1d(3, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cv3): Conv1d(64, 6, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (4): NN(\n",
      "      (cv1): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cv3): Conv1d(64, 4, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (5): NN(\n",
      "      (cv1): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cv3): Conv1d(64, 4, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (6): NN(\n",
      "      (cv1): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cv3): Conv1d(64, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (7): NN(\n",
      "      (cv1): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (cv3): Conv1d(64, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "  )\n",
      "  (convinv): ModuleList(\n",
      "    (0): Invertible1x1Conv(\n",
      "      (conv): Conv1d(8, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    )\n",
      "    (1): Invertible1x1Conv(\n",
      "      (conv): Conv1d(8, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    )\n",
      "    (2): Invertible1x1Conv(\n",
      "      (conv): Conv1d(6, 6, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    )\n",
      "    (3): Invertible1x1Conv(\n",
      "      (conv): Conv1d(6, 6, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    )\n",
      "    (4): Invertible1x1Conv(\n",
      "      (conv): Conv1d(4, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    )\n",
      "    (5): Invertible1x1Conv(\n",
      "      (conv): Conv1d(4, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    )\n",
      "    (6): Invertible1x1Conv(\n",
      "      (conv): Conv1d(2, 2, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    )\n",
      "    (7): Invertible1x1Conv(\n",
      "      (conv): Conv1d(2, 2, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Loss function:\n",
      " WaveGlowLoss()\n",
      "\n",
      "Optimizer:\n",
      " Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = WaveGlow(8, 8, 2, 2)\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = WaveGlowLoss()\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "print('The number of trainable parameters:', num_trainable_params)\n",
    "print('\\nModel:\\n', net)\n",
    "print('\\nLoss function:\\n', criterion)\n",
    "print('\\nOptimizer:\\n', optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ネットワークを実体化、ロス関数とオプティマイザを定義\n",
    "net = CNN_CIFAR10()\n",
    "net = net.to(device) # .to(device)でCPU/GPUに送信できる\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # クロスエントロピー誤差. ソフトマックスも含む\n",
    "\n",
    "learning_rate = 0.001\n",
    "# オプティマイザはAdam.\n",
    "# パラメータを渡し、学習率、weight decayの係数を指定（これはoptional）\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "\n",
    "# モデルのtarinableな(勾配を要求する)パラメータの数をカウントする（.numel()で要素数の合計がわかる）\n",
    "num_trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "# モデルの構造、ロス関数、オプティマイザの表示\n",
    "print('The number of trainable parameters:', num_trainable_params)\n",
    "print('\\nModel:\\n', net)\n",
    "print('\\nLoss function:\\n', loss_fn)\n",
    "print('\\nOptimizer:\\n', optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    net.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for x in train_loader:\n",
    "        x = x.to(device)\n",
    "        model_output = net(x)\n",
    "        \n",
    "        loss = criterion(model_output)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(cifar10_train_loader)\n",
    "    validation_loss, validation_accuracy = validation(cifar10_validation_loader)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    validation_loss_list.append(validation_loss)\n",
    "    validation_accuracy_list.append(validation_accuracy)\n",
    "    \n",
    "    print('epoch[%2d/%2d] loss:%1.4f validation_loss:%1.4f validation_accuracy:%1.4f' % \\\n",
    "                            (epoch+1, n_epochs, train_loss, validation_loss, validation_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]])\n",
      "tensor([[[ 0,  8, 16, 24, 32, 40, 48, 56],\n",
      "         [ 1,  9, 17, 25, 33, 41, 49, 57],\n",
      "         [ 2, 10, 18, 26, 34, 42, 50, 58],\n",
      "         [ 3, 11, 19, 27, 35, 43, 51, 59],\n",
      "         [ 4, 12, 20, 28, 36, 44, 52, 60],\n",
      "         [ 5, 13, 21, 29, 37, 45, 53, 61],\n",
      "         [ 6, 14, 22, 30, 38, 46, 54, 62],\n",
      "         [ 7, 15, 23, 31, 39, 47, 55, 63]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(64).view(1,-1)\n",
    "y = x.unfold(1,8,8).permute(0,2,1)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.permute(0,2,1).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_flow_invertibity(net, inputs, epsilon=1e-5):\n",
    "    with torch.no_grad():    \n",
    "        recons1 = net.inverse(net.forward(inputs)[0])\n",
    "        recons2 = net.forward(net.inverse(inputs))[0]\n",
    "    \n",
    "        diff1 = torch.abs(inputs - recons1)\n",
    "        diff2 = torch.abs(inputs - recons2)\n",
    "    \n",
    "        print('forward -> inverse:', not (diff1 > epsilon).any().item())\n",
    "        print('inverse -> forward:', not (diff2 > epsilon).any().item())\n",
    "        \n",
    "    for i in range(50):\n",
    "        optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
    "        criterion = nn.MSELoss()\n",
    "        outputs = net(inputs)[0]\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        recons1 = net.inverse(net.forward(inputs)[0])\n",
    "        recons2 = net.forward(net.inverse(inputs))[0]\n",
    "    \n",
    "        diff1 = torch.abs(inputs - recons1)\n",
    "        diff2 = torch.abs(inputs - recons2)\n",
    "    \n",
    "        print('forward -> inverse(trained):', not (diff1 > epsilon).any().item())\n",
    "        print('inverse -> forward(trained):', not (diff2 > epsilon).any().item())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
